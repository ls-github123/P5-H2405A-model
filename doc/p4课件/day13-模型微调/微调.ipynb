{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础知识\n",
    "\n",
    "## NVIDIA\n",
    "### CUDA\n",
    "\n",
    "CUDA是啥\n",
    "\n",
    "\n",
    "```\n",
    "查看显存命令，也可以查看cuda版本\n",
    "nvidia-smi\n",
    "```\n",
    "## AMD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在线GPU环境\n",
    "\n",
    "## 阿里云PAI\n",
    "\n",
    "每天发送500份\n",
    "\n",
    "### PAI-DSW\n",
    "\n",
    "执行下面命令，然后就可以访问端口了\n",
    "\n",
    "export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/\n",
    "\n",
    "浏览器打开\n",
    "\n",
    "https://dsw-gateway-cn-shanghai.data.aliyun.com/dsw-362554/proxy/7860/\n",
    "\n",
    "## 魔搭社区\n",
    "\n",
    "注册后有免费的100小时的GPU资源\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qwen 微调法律大模型\n",
    "\n",
    "## 准备环境\n",
    "\n",
    "```\n",
    "# 下载qwen和模型\n",
    "git clone https://github.com/QwenLM/Qwen.git\n",
    "git clone https://www.modelscope.cn/qwen/Qwen-1_8B-Chat.git\n",
    "\n",
    "# 安装依赖\n",
    "cd Qwen\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 安装torch\n",
    "方法1\n",
    "下载地址：http://download.pytorch.org/whl/torch_stable.html\n",
    "wget http://download.pytorch.org/whl/cu121/torch/torch-2.1.1+cu121-cp310-cp310-linux_x86_64.whl\n",
    "pip install torch-2.1.1+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "方法2\n",
    "去这个网站上找好对应版本的指令：https://pytorch.org/get-started/previous-versions/\n",
    "pip install torch==2.1.1+cu121 torchvision==0.15.2+cu121 torchaudio==2.1.1+cu121 torchtext==0.15.2 torchdata==0.6.10 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "\n",
    "# 安装deepspeed\n",
    "pip install \"peft<0.8.0\" deepspeed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调\n",
    "\n",
    "准备模型：Qwen-1_8B-Chat\n",
    "\n",
    "准备训练数据:trans_data.json\n",
    "\n",
    "```\n",
    "cd Qwen\n",
    "\n",
    "# 移动finetune_lora_single_gpu.sh到外面\n",
    "cp finetune/finetune_lora_single_gpu.sh ./\n",
    "chmod +x finetune_lora_single_gpu.sh\n",
    "\n",
    "# 修改微调配置\n",
    "vim finetune_lora_single_gpu.sh\n",
    "\n",
    "\n",
    "# 微调，根据需要修改\n",
    "./finetune_lora_single_gpu.sh -m /mnt/workspace/Qwen/Qwen-1_8B-Chat -d /mnt/workspace/data/trans_data.json\n",
    "\n",
    "# 合并模型，模型合并文件 qwen_lora_merge.py在钉钉的文件夹里，上传到Qwen文件夹里，然后修改微调模型路径\n",
    "lora_model_path=\"/mnt/workspace/Qwen/output_qwen/checkpoint-30\"\n",
    "\n",
    "# 合并模型\n",
    "python qwen_lora_merge.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "### 交互式Demo\n",
    "```\n",
    "cd Qwen\n",
    "python cli_demo.py -c Qwen-1_8B-Chat_merge\n",
    "```\n",
    "\n",
    "```\n",
    "# 启动服务\n",
    "python web_demo.py --cpu-only -c=Qwen-1_8B-Chat\n",
    "# 查看web界面\n",
    "http://127.0.0.1:8000/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "1、ImportError: cannot import name 'packaging' from 'pkg_resources' \n",
    "```\n",
    "ImportError: cannot import name 'packaging' from 'pkg_resources' (/ibex/user/xxxx/mambaforge/envs/trivol/lib/python3.9/site-packages/pkg_resources/__init__.py)\n",
    "\n",
    "用pip list | grep setuptools 查了一下，steuptools是 70.0.0版本。卸载重装为69.5.1版本即解决问题。\n",
    "\n",
    "解决方案：\n",
    "pip uninstall setuptools\n",
    "pip install setuptools==69.5.1 --no-cache-dir\n",
    "```\n",
    "\n",
    "2、ValueError: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\n",
    "```\n",
    "解决方案：\n",
    "需要修改finetune_lora_single_gpu.sh文件，最下面有说明\n",
    "--bf16 True  修改为 --fp16 True --deepspeed finetune/ds_config_zero2.json\n",
    "```\n",
    "\n",
    "3、TypeError: isin() received an invalid combination of arguments - got \n",
    "```\n",
    "解决方案：\n",
    "pip install coqui-tts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qwen2 微调大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaFactory微调大模型\n",
    "\n",
    "## 安装\n",
    "\n",
    "```\n",
    "git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "\n",
    "# 安装torch\n",
    "# wget http://download.pytorch.org/whl/cu121/torch-2.1.1%2Bcu121-cp310-cp310-linux_x86_64.whl\n",
    "# pip install torch-2.1.1+cu121-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "cd LLaMA-Factory\n",
    "\n",
    "pip install -e.[torch,metrics]\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "## 上传数据集\n",
    "\n",
    "上传到LLaMA-Factory/data/目录，并修改同目录下的dataset_info.json，修改如下：\n",
    "\n",
    "```\n",
    "  \"mydata\": {\n",
    "    \"file_name\": \"trans_data.json\",\n",
    "    \"formatting\":\"sharegpt\",\n",
    "    \"columns\": {\n",
    "        \"messages\": \"conversations\",\n",
    "        \"tools\":\"id\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "        \"role_tag\": \"from\",\n",
    "        \"content_tag\": \"value\",\n",
    "        \"user_tag\": \"user\",\n",
    "        \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "```\n",
    "\n",
    "## 微调-web\n",
    "\n",
    "pip install tf-keras\n",
    "\n",
    "export GRADIO_ROOT_PATH=/${JUPYTER_NAME}/proxy/7860/\n",
    "\n",
    "https://dsw-gateway-cn-shanghai.data.aliyun.com/dsw-362554/proxy/7860/\n",
    "\n",
    "python src/webui.py\n",
    "\n",
    "功能包括：微调train、测试chat、导出模型export\n",
    "\n",
    "## 部署\n",
    "\n",
    "同qwen的部署方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见问题\n",
    "\n",
    "```\n",
    "1、/bin/sh: 1: llamafactory-cli: not found\n",
    "解决方案：\n",
    "运行以下代码，然后重新安装依赖\n",
    "pip install -e.[metrics]\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
