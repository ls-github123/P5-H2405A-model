{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b54a184-b1a0-4125-8578-729c9410ff58",
   "metadata": {},
   "source": [
    "# 模型社区\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "官网：https://huggingface.co/\n",
    "\n",
    "HuggingFace 是一个自然语言处理（NLP）领域的开源社区和平台，它提供了一系列强大的工具、库和预训练模型，帮助开发者快速构建和部署自然语言处理应用。HuggingFace 平台的主要组成部分和特点如下：\n",
    "\n",
    "1. **Transformers 库**：HuggingFace 的 Transformers 库是其最著名和核心的部分。它提供了广泛的预训练模型（如BERT、GPT、RoBERTa等）的实现，并提供易于使用的API，用于进行文本分类、命名实体识别、文本生成等各种 NLP 任务。Transformers 库支持多种主流深度学习框架，如PyTorch和TensorFlow。\n",
    "2. **模型架构和优化方法**：HuggingFace 提供了各种用于构建和优化 NLP 模型的架构和方法，包括用于序列分类、序列标注、文本生成等任务的模型架构和损失函数，以及用于模型训练和优化的技术，如学习率调度、权重衰减等。\n",
    "3. **数据集和指标**：HuggingFace 提供了大量的 NLP 数据集，用于训练和评估模型。这些数据集涵盖了各种不同的任务和语言，包括文本分类、命名实体识别、情感分析等。此外，HuggingFace 还提供了常用的评估指标和评估方法，帮助用户对模型性能进行评估和比较。\n",
    "4. **模型训练和部署工具**：HuggingFace 提供了用于模型训练和部署的工具和库，使用户能够轻松地进行模型训练、微调和部署。例如，通过使用 HuggingFace 的 Trainer 类，用户可以更便捷地配置和执行模型的训练过程。\n",
    "5. **模型分享和社区**：HuggingFace 平台鼓励用户分享和交流模型、代码和经验。用户可以在 HuggingFace 的模型仓库中发布和共享自己的模型，并从社区中获取模型、代码和应用案例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdae68-9ab7-4354-a4ca-450fedbc38f9",
   "metadata": {},
   "source": [
    "## ModelScope\n",
    "\n",
    "官网：https://www.modelscope.cn\n",
    "\n",
    "魔搭社区ModelScope是一个由阿里达摩院推出的开源模型服务平台，其主要功能和目的如下：\n",
    "\n",
    "1. 模型共享与探索： ModelScope汇集了各领域最先进的机器学习模型，包括但不限于自然语言处理、计算机视觉、语音识别等。用户可以在平台上发现和探索这些模型，了解其特性和性能。\n",
    "2. 一站式服务： 提供从模型探索、推理、训练到部署和应用的一站式服务。用户不仅可以体验预训练模型的性能，还可以根据自己的需求对模型进行定制和训练，并方便地将训练好的模型部署到实际应用中。\n",
    "3. 易用性和灵活性： ModelScope旨在为泛AI开发者提供灵活、易用、低成本的模型服务产品。用户无需额外部署复杂的环境，就可以在平台上直接使用各种模型，降低了使用和开发AI模型的门槛。\n",
    "4. 开源与合作： 作为一款开源平台，ModelScope鼓励社区成员参与模型的开发、改进和分享。通过共同合作，推动AI技术的发展和创新。\n",
    "5. 智能体开发框架： ModelScope还推出了ModelScope-Agent开发框架，如MSAgent-Qwen-7B，允许用户打造属于自己的智能体。这个框架提供了丰富的环境配置选项，支持单卡运行，并有一定的显存要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4458ce0-abca-4cd6-8f56-ca7104879b2f",
   "metadata": {},
   "source": [
    "## AutoDL \n",
    "- https://www.autodl.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173092c-8a21-46f4-8494-6680078c75da",
   "metadata": {},
   "source": [
    "### 学术加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becefba-962f-4f3c-9a7b-79091282f6af",
   "metadata": {},
   "source": [
    "- 访问github等外网速度慢时可开启学术加速\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00474b6-eecc-458c-b55b-28a6296239e0",
   "metadata": {},
   "source": [
    "```\n",
    "开启学术加速\n",
    "source /etc/network_turbo\n",
    "\n",
    "取消学术加速，如果不再需要建议关闭学术加速，因为该加速可能对正常网络造成一定影响\n",
    "unset http_proxy && unset https_proxy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8a811-2939-4d65-8a0d-8cf47acd19a5",
   "metadata": {},
   "source": [
    "### 磁盘扩容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb42c9-aa72-49fe-b6bd-0ecb28c36f3f",
   "metadata": {},
   "source": [
    "- 这里扩容了100G，容量自动扩展到/root/autodl-tmp目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d1766-8b4e-4ba3-9567-76dfb1b3b89c",
   "metadata": {},
   "source": [
    "```\n",
    "存储：\n",
    "  系 统 盘/               ：32% 9.5G/30G\n",
    "  数 据 盘/root/autodl-tmp：1% 8.0K/150G\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f661b-efbd-45f5-ac92-4a0839d50d3e",
   "metadata": {},
   "source": [
    "- 若使用过程中发现磁盘空间不足，可关机后扩容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e3ff5-f814-4240-8216-ecc400928355",
   "metadata": {},
   "source": [
    "<img src=\"img/xx_20240803100734.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb88bc-203c-4872-8907-ca5f56dac773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb57015e-f9ee-4f74-bb05-ab8ce3c0accd",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/dd_20240803100507.png\" style=\"margin-left: 0px\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74907e9-d228-40cc-8a23-a147cac3d57b",
   "metadata": {},
   "source": [
    "## 使用ollama3部署本地大模型\n",
    "Ollama 是一个开源项目，它允许用户在本地机器上运行大型语言模型。这个项目简化了在个人电脑或服务器上部署和运行像LLaMA这样的大型语言模型的过程。如果你想要使用 Ollama 部署本地的大模型\n",
    "\n",
    "- 由于学生电脑系统不统一，有的是windows，有的是mac,还有的是Ubautu\n",
    "- 这几个系统中，Ubautu遇到的问题比较少，windows可能遇到的问题最多\n",
    "- 给学生一定时间自行安装，遇到问题首先尝试自行搜索解决"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6431b8-6642-4b2c-9e84-709c085dc02f",
   "metadata": {},
   "source": [
    "下载对应系统的版本\n",
    "- https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc3ca7-b46e-4a56-924a-6e8fa70d1423",
   "metadata": {},
   "source": [
    "### windows安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029dec-0ea4-4d5e-b567-8c3bfb24f4e5",
   "metadata": {},
   "source": [
    "- 下载完毕后，双击执行安装\n",
    "- 安装完成后在cmd中执行以下命令，安装具体某个大模型\n",
    "- 如果本地不存在模型，则会先行下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48621d6-c7ec-49f1-b5c5-18967612bf07",
   "metadata": {},
   "source": [
    "linux安装\n",
    "\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "<img src=\"img/lla_20240801143507.png\" style=\"margin-left: 0px\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746bf65-9804-4ed0-8b43-ab19c2eed6a0",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "PS C:\\> ollama list\n",
    "NAME                    ID              SIZE    MODIFIED\n",
    "llama2-chinese:13b      990f930d55c5    7.4 GB  6 weeks ago\n",
    "gemma:latest            a72c7f4d0a15    5.0 GB  6 weeks ago\n",
    "qwen2:latest            e0d4e1163c58    4.4 GB  6 weeks ago\n",
    "llama3:latest           365c0bd3c000    4.7 GB  6 weeks ago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce344a-833c-45d1-a8d7-b57e07c1a9c0",
   "metadata": {},
   "source": [
    "### 测试服务是否启动\n",
    "- http://localhost:11434/api/tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46527ea5-c7d3-4277-9783-a42333cc91a1",
   "metadata": {},
   "source": [
    "正常状态如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c22676-42e0-42e0-bb3b-639a3ac5a32e",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/lla_20240801145720.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama serve\n",
    "ollama run llama3.1\n",
    "\n",
    "测试代码\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.1\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ]\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d63a7e-0910-42a6-8269-abd12679193b",
   "metadata": {},
   "source": [
    "### llama3支持的大模型\n",
    "- https://github.com/ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb60fe-375b-415b-ac56-474943b753fb",
   "metadata": {},
   "source": [
    "## REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18daf6d-5fa2-432c-8c24-bef6a2771581",
   "metadata": {},
   "source": [
    "- windows上可以安装git，这样就可以使用curl命令，以方便验证ollama服务是否正常启动\n",
    "- 初学者不要WSL中运行，因为网络的原因默认wsl中无法访问本地上安装的ollama服务\n",
    "- 初学者也不要在docker中访问主机上的ollama，很可能因为网络不通而无法访问\n",
    "- mac,ubautu系统可直接执行以下命令"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e20f8e-acc4-486b-ba31-0c2be2ec7658",
   "metadata": {},
   "source": [
    "### Generate a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5235b-d860-428b-aa69-291e33907ca9",
   "metadata": {},
   "source": [
    " \n",
    "```\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\":\"Why is the sky blue?\"\n",
    "    }'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadf395-a0e6-42ea-8e57-e8136d90a451",
   "metadata": {},
   "source": [
    "### Chat with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecf8a1-6633-4ad1-99eb-95764be8390a",
   "metadata": {},
   "source": [
    "```\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "    \"model\": \"llama3\",\n",
    "    \"messages\": [\n",
    "        { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "    ]\n",
    "    }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e901b-d594-4120-8525-81e78e12d31b",
   "metadata": {},
   "source": [
    "## 使用工具访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2792967-8b8a-4d39-8266-9e462d3e90f5",
   "metadata": {},
   "source": [
    "### apipost\n",
    "- https://www.apipost.cn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495f7a4-8b58-4302-818c-4f09285c40bf",
   "metadata": {},
   "source": [
    "<img src=\"img/api_20240802082845.png\" style=\"margin-left: 0px\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0efa6-5102-47f8-a1ad-346ef352dad0",
   "metadata": {},
   "source": [
    "使用apipost/postman \n",
    "apipost 设置 -- 超时时间要设置长一些"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0e790-53b5-4869-86c0-1d19cc2eb788",
   "metadata": {},
   "source": [
    "<img src=\"img/ll_20240802082504.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b2c4a-3369-4d82-9651-edf1dbbb7c64",
   "metadata": {},
   "source": [
    "### postman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd375804-7fef-4e4f-a189-539e56ae6dd9",
   "metadata": {},
   "source": [
    "- https://www.postman.com/downloads/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f6d30-3a4a-48ff-a6d4-5fd5029c3c7e",
   "metadata": {},
   "source": [
    "<img src=\"img/post_20240802083145.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f945374-f9c6-48e9-baf6-b53d7b3ae00d",
   "metadata": {},
   "source": [
    "<img src=\"img/post_20240802084235.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5d349-89ee-4e34-b7ea-b394e68cfa04",
   "metadata": {},
   "source": [
    "## python post请求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b72698-65cb-4398-9ae8-d280bf32d94b",
   "metadata": {},
   "source": [
    "### 本地post请求示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26014c1-7055-4f65-8173-90c4575ec4bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"id\":\"chatcmpl-216\",\"object\":\"chat.completion\",\"created\":1722559669,\"model\":\"llama3:latest\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"I'm just an AI, a helpful assistant designed to assist and communicate with you in a way that's easy to understand and interact with. I can answer questions, provide information, generate ideas, and even have a conversation or tell a story. My purpose is to help make your life easier, more efficient, and more enjoyable!\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":0,\"completion_tokens\":67,\"total_tokens\":67}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "  \n",
    "url = 'http://127.0.0.1:11434/v1/chat/completions'    \n",
    "  \n",
    "# 发送JSON数据  \n",
    "json_data = {\n",
    "    'model': 'llama3:latest',\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful assistant.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '你是谁？'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "  \n",
    "# 发送POST请求，并指定json参数  \n",
    "response = requests.post(url, json=json_data)  \n",
    "  \n",
    "print(response.status_code)  \n",
    "print(response.text)  #str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ed178-84c2-40fd-b8b2-b680dbcfee76",
   "metadata": {},
   "source": [
    "- 增加状态码处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a78d6024-f058-4a54-b2c9-4274106dfb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-207\",\"object\":\"chat.completion\",\"created\":1722559818,\"model\":\"qwen2:latest\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"我是阿里云开发的一款超大规模语言模型，我叫通义千问。作为一个预训练语言模型，我的全名是“通义千问”，是由阿里云自主研发的超大规模语言模型。如果您有任何其他想要了解的内容，欢迎您进一步提问！\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":23,\"completion_tokens\":55,\"total_tokens\":78}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from requests.exceptions import RequestException  \n",
    "  \n",
    "url = 'http://127.0.0.1:11434/v1/chat/completions'  \n",
    "# 发送JSON数据  \n",
    "json_data = {\n",
    "    'model': 'qwen2:latest',\n",
    "    'messages': [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful assistant.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '你是谁？'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "  \n",
    "try:  \n",
    "    response = requests.post(url, json=json_data)  \n",
    "    response.raise_for_status()  # 如果响应状态码不是200，将抛出HTTPError异常  \n",
    "    print(response.text)  \n",
    "except RequestException as e:  \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e768a-dd8c-4cb2-87b4-13ea56778f31",
   "metadata": {},
   "source": [
    "### 方法封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed7219c-a682-4af0-be83-2fcada319d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "import json \n",
    "from requests.exceptions import RequestException  \n",
    "\n",
    "def local_llm(query,model_name=\"qwen2:latest\"):\n",
    "    url = 'http://127.0.0.1:11434/v1/chat/completions'  \n",
    "    # 假设我们需要发送JSON数据  \n",
    "    json_data = {\n",
    "        'model': model_name,\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are a helpful assistant.'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': query\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "      \n",
    "    try:  \n",
    "        response = requests.post(url, json=json_data)  \n",
    "        response.raise_for_status()  # 如果响应状态码不是200，将抛出HTTPError异常  \n",
    "        # print(response.text)\n",
    "        data_json = json.loads(response.text)\n",
    "        return data_json[\"choices\"][0][\"message\"]\n",
    "    except RequestException as e:  \n",
    "        print(e)\n",
    "    return {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58294c9-cbf8-4565-be1a-8a4666db14bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一只猫看见另一只猫骑自行车，于是它也去学，但是怎么也学不会。另一只猫问：“你为什么这么笨？”这只猫回答说，“我可能上辈子欠了什么债，这辈子在还。”\n"
     ]
    }
   ],
   "source": [
    "res = local_llm(\"讲个笑话\",model_name=\"qwen2:latest\")\n",
    "print(res['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06aac0b4-0078-4eef-b704-9eb1785d3acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh! 😄 Do you want another one?\n"
     ]
    }
   ],
   "source": [
    "res = local_llm(\"讲个笑话\",model_name=\"llama3:latest\")\n",
    "print(res['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58aad3dd-aac3-45ec-b55f-5f8c84f102f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "为什么程序员的女朋友总是会说：“我爱你，但我更爱我的代码”？\n",
      "\n",
      "因为他们认为，代码比人更加稳定、可靠、不容易崩溃！😄\n"
     ]
    }
   ],
   "source": [
    "res = local_llm(\"讲个笑话，关于程序员的，请使用中文\",model_name=\"llama3:latest\")\n",
    "print(res['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07ba3b-f601-4fbb-a644-8acb41a5b82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "addd0f18-9903-4341-93ce-f852999575e7",
   "metadata": {},
   "source": [
    "## WebUI调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559f9c4-a2d2-421e-8207-923277e77de9",
   "metadata": {},
   "source": [
    "### Docker Desktop下载\n",
    "- https://docs.docker.com/desktop/release-notes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc303ee-0ff6-4169-a756-85536f15eff5",
   "metadata": {},
   "source": [
    "### openwebui\n",
    "- https://openwebui.com/\n",
    "- 下载docker镜像\n",
    "- https://github.com/open-webui/open-webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d9a75-b949-4748-b41a-8221c41534bf",
   "metadata": {},
   "source": [
    "### docker镜像安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a62a2-32e9-4855-a59b-7fb58a4c0188",
   "metadata": {},
   "source": [
    "```\n",
    "GPU \n",
    "docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n",
    "\n",
    "CPU \n",
    "docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n",
    "\n",
    "这种方式不是IP映射，而是直接将ollama文件映射到docker中，这种方式不会出现docker中服务已启动但外部wondows却无法访问的现象\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bae05-ec86-4005-ae4b-792ea0a33c55",
   "metadata": {},
   "source": [
    "常见问题\n",
    "- 正常情况下，初学者会卡在这里，出现各种异常情况\n",
    "- 上面的安装方式只是避免windows系统的网络问题，mac上的同学不必如此，但mac上的docker也有自身的网络问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7efa5-a247-43fc-a04a-7f64802d9a2e",
   "metadata": {},
   "source": [
    "```\n",
    "If Ollama is on your computer, use this command:\n",
    "docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n",
    "\n",
    "To run Open WebUI with Nvidia GPU support, use this command:\n",
    "docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n",
    "\n",
    "\n",
    "\n",
    "If Ollama is on a Different Server, use this command:\n",
    "To connect to Ollama on another server, change the OLLAMA_BASE_URL to the server's URL:\n",
    "\n",
    "docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa246d-af56-48c9-9df2-1cbc2aa30a54",
   "metadata": {},
   "source": [
    "- 通过apipost/postman可以访问ollama，确认Ollama正在运行，但WebUI不识别\n",
    "- 这是WebUI Docker创建时的网络方式导致的，以open-webui为例 \n",
    "- https://github.com/open-webui/open-webui#troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46de15-395b-4d91-816e-5ee2b5f2e0bf",
   "metadata": {},
   "source": [
    "### openwebui访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9537d8-cdb5-45a1-b85a-c20330c59379",
   "metadata": {},
   "source": [
    "- 启动docker实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6356957-4638-4244-9df1-d95c352ae3fc",
   "metadata": {},
   "source": [
    "<img src=\"img/ui_20240802093919.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848be929-7286-461e-b08d-c6416a4397f3",
   "metadata": {},
   "source": [
    "- http://localhost:3000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4790831-338b-4b34-a575-68b6c9aa9c23",
   "metadata": {},
   "source": [
    "<img src=\"img/set_20240802094043.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789ab3f-14fa-40f2-a6e1-8caeef3f9785",
   "metadata": {},
   "source": [
    "先拉取模型再选择模型\n",
    "- 虽然ollama已经下载了模型，但有时候该webui只识别在自己平台下载的模型\n",
    "- 出现此现象的原因，大概率是网络不畅导致的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcdd69-9e4b-4609-8527-1abbf7a3bbab",
   "metadata": {},
   "source": [
    "<img src=\"img/ml_20240802094239.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e276f-d86a-4c59-b777-e24b5d53111c",
   "metadata": {},
   "source": [
    "正常情况下，比如网络畅通时，哪怕不下载也会展示一系列的大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810da92-651a-4655-97a4-ce7e3e424403",
   "metadata": {},
   "source": [
    "<img src=\"img/ui_20240802094646.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8c8c8-21d3-430e-a182-e1ccc4367210",
   "metadata": {},
   "source": [
    "### 对话示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddd4db-3a0a-49e1-81a9-2a589ac2f43c",
   "metadata": {},
   "source": [
    "<img src=\"img/shi_20240802095116.png\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac72ed-59d9-497f-b15c-ffa91492296e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecb6755-e5ee-4a66-9260-41d6dc0d8134",
   "metadata": {},
   "source": [
    "### lobe chat ui\n",
    "- 选做，根据自身情况，尝试阅读官方说明文档，独立完成安装部署\n",
    "  - 首先尝试官方文档入手，一步步操作，因为这是第一手资料，面试中有一项加分项是跟踪最新前沿技术\n",
    "  - 若失败，则尝试搜索或者其他视频网站(比如B站)搜索相关的教程\n",
    "  - 再失败，请教老师，同学\n",
    "- https://lobehub.com/zh/docs/usage/features/local-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbe7e6-3a00-420e-9788-70b57e924133",
   "metadata": {},
   "source": [
    "## ChatGLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb3694-f8d2-42a8-87a8-aee203f3606b",
   "metadata": {},
   "source": [
    "### 9B系列简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d91bb-cafc-4bc3-bec8-2d8d17e0d0e8",
   "metadata": {},
   "source": [
    "```\n",
    "GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本。 \n",
    "\n",
    "在语义、数学、推理、代码和知识等多方面的数据集测评中，\n",
    "GLM-4-9B 及其人类偏好对齐的版本 GLM-4-9B-Chat 均表现出较高的性能。 \n",
    "\n",
    "除了能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、\n",
    "自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。 \n",
    "\n",
    "本代模型增加了多语言支持，支持包括日语，韩语，德语在内的 26 种语言。\n",
    "我们还推出了支持 1M 上下文长度（约 200 万中文字符）的模型。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe97ef-7577-4f82-b1d8-594422a7c9e9",
   "metadata": {},
   "source": [
    "<img src=\"img/glm_20240802114923.jpg\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695e301-190e-4894-9987-1cc316713a9c",
   "metadata": {},
   "source": [
    "<img src=\"img/glm_20240802110632.jpg\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24bcb0-d864-4c1c-9372-a3a880dc0361",
   "metadata": {},
   "source": [
    "### 评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45999800-8ba7-4926-a787-175811668805",
   "metadata": {},
   "source": [
    "- MMLU(核心)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffe73b-c987-43d5-9242-d9976410d32c",
   "metadata": {},
   "source": [
    "```\n",
    "Massive 英/ˈmæsɪv/ 美/ˈmæsɪv/  adj.大量的，巨大的，大规模的；\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7192fd-3bff-4acc-b36d-9b57cab015b4",
   "metadata": {},
   "source": [
    "```\n",
    "含义：\n",
    "MMLU（Massive Multitask Language Understanding）基准测试旨在评估大语言模型在多种自然语言理解任务上的性能。它通常包含一系列多样化的任务，如阅读理解、常识推理、文本分类等，以全面衡量模型的能力。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448d06a-9c99-4da5-904e-b690a6959ecc",
   "metadata": {},
   "source": [
    "```\n",
    "特点：\n",
    "多任务：涵盖多种自然语言理解任务。\n",
    "广泛覆盖：能够评估模型在不同领域和任务上的泛化能力。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98ac40-3217-4c40-b3cb-e63f91bd892e",
   "metadata": {},
   "source": [
    "- C-Eval(中文)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d8c86-1809-4100-bb98-417a0061b87c",
   "metadata": {},
   "source": [
    "```\n",
    "含义：\n",
    "C-Eval是一个针对中文大模型的知识和推理能力评估基准。\n",
    "它包含覆盖人文、社科、理工等多个学科方向的题目，\n",
    "旨在测试模型在知识型任务和推理任务上的表现。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d2e15-414d-45e3-b9d9-83dbfc7724a1",
   "metadata": {},
   "source": [
    "```\n",
    "特点：\n",
    "针对性强：专注于中文大模型的评估。\n",
    "广泛覆盖：包含多个学科方向的题目。\n",
    "区分度高：能够区分模型在知识和推理能力上的强弱。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f3d9b-a63a-4424-896c-15fa638a0f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2c27e26-bb2b-4be3-9fe7-9a408f61bc8d",
   "metadata": {},
   "source": [
    "### 硬件配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037af0d-2170-4dfd-a612-0e669d0afadb",
   "metadata": {},
   "source": [
    "- 依输入长度去选配置，默认8K输入，选24G显存\n",
    "- 量化为INT4后，8K输入在12G显存上就可以运行，这也是本地运行通常做量化的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6b0de-c3b1-41cd-b1fd-bec1ed2c75bd",
   "metadata": {},
   "source": [
    "这里默认是8K，所以要选一个24G的显卡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d788b9-b12b-4701-bcf4-b6535a43898e",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/glm_20240802115707.jpg\" style=\"margin-left: 0px\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa911d-a863-4362-a8f5-2226b5fe328b",
   "metadata": {},
   "source": [
    "### 下载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b641165-ec80-4a7b-a84a-ee94387e2d9a",
   "metadata": {},
   "source": [
    "API调用示例下载\n",
    "- https://github.com/THUDM/GLM-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19df30-2538-4fda-b0d9-bc2e313c9cd5",
   "metadata": {},
   "source": [
    "```\n",
    "git clone https://github.com/THUDM/GLM-4.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66015827-5bae-4335-8eaa-33405df3a3e3",
   "metadata": {},
   "source": [
    "模型文件下载\n",
    "- https://huggingface.co/THUDM/glm-4-9b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd8dd1-ba70-43ad-a882-427a9ec6a729",
   "metadata": {},
   "source": [
    "```\n",
    "apt install curl git\n",
    "curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "apt install git-lfs\n",
    "\n",
    "git lfs install\n",
    "cd /root/autodl-tmp\n",
    "nohup git clone https://www.modelscope.cn/models/ZhipuAI/glm-4-9b-chat.git >/tmp/git.log 2>&1 &\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d5e9c-299c-4fca-adb3-ca8ccc8ec61f",
   "metadata": {},
   "source": [
    "### 依赖安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30379a-1b51-48e7-86f8-52302ef31fa7",
   "metadata": {},
   "source": [
    "- 原生服务器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14064ead-e7e3-4ccf-9b75-90469f32bdb2",
   "metadata": {},
   "source": [
    "```\n",
    "cd basic_demo\n",
    "pip install requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ab266-0b04-4d97-a951-220b35ba8ee3",
   "metadata": {},
   "source": [
    "- 社区服务器，补充安装，或者运行时缺少什么就再安装什么,比如"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f153b-3797-4129-bdf2-23f5eca8c38c",
   "metadata": {},
   "source": [
    "```\n",
    "pip install uvicorn\n",
    "pip install vllm\n",
    "pip install sse_starlette\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8509b5-f218-46a1-b59f-d280f7117f1d",
   "metadata": {},
   "source": [
    "### Uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f934e-6af3-4fe4-8b8a-1b6895ff0f91",
   "metadata": {},
   "source": [
    "```\n",
    "Uvicorn 是一个轻量级的 ASGI（Asynchronous Server Gateway Interface）服务器，它用于运行 Python 3.7+ 编写的异步 Web 应用程序。ASGI 是一种标准，允许 Python 框架和服务器之间异步通信，支持 WebSocket 和 HTTP/2 等现代网络协议。Uvicorn 因其高性能和易用性而广受欢迎，特别是在需要处理大量并发连接或需要高性能的异步 Web 应用程序中\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd04cb3-3aed-4ffd-b731-642b9d2bfc87",
   "metadata": {},
   "source": [
    "### vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b015db-2e29-4914-9506-14623d796d0e",
   "metadata": {},
   "source": [
    "- vLLM应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310f604-179b-433a-b7ff-62bd2265eb93",
   "metadata": {},
   "source": [
    "```\n",
    "vLLM适用于需要高效大模型推理的各种场景，如自然语言处理、聊天机器人、文本生成等。通过vLLM，开发者可以轻松地实现高性能、低延迟的模型推理服务。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614f769-88c3-46f4-b727-84635c9fc3b3",
   "metadata": {},
   "source": [
    "- vLLM的特点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca650722-bb60-4606-9e8a-d147b55bdc57",
   "metadata": {},
   "source": [
    "```\n",
    "Paged Attention：vLLM首次提出并实现了Paged Attention算法，该算法借鉴了操作系统的分页管理概念，将连续的键值缓存（KV Cache）分散存储，以减少显存的不必要占用，提高整体性能。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a4650-fcd2-44d5-bb1f-890ffd40756a",
   "metadata": {},
   "source": [
    "```\n",
    "Continuous Batching：vLLM通过连续批次处理（Continuous Batching）策略，即在生成一个token后立即安排下一批请求，来提升推理效率。这种策略允许vLLM在每一轮迭代中处理不同数量的请求（即batch size不固定），从而最大化吞吐量。\n",
    "多种优化技术：\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eecc7c-4072-41b6-be7a-58757ad73b9d",
   "metadata": {},
   "source": [
    "```\n",
    "vLLM还支持量化（如GPTQ、AWQ、SqueezeLLM、FP8 KV Cache等）、Tensor Parallelism、高性能CUDA kernel等功能，以进一步优化推理性能。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36647f3-5f11-465d-8187-1b1dd353b8d3",
   "metadata": {},
   "source": [
    "### 服务启动"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f020e-1bfb-4c97-a116-631a9aa44869",
   "metadata": {},
   "source": [
    "```\n",
    "cd /root/autodl-tmp/GLM-4/basic_demo\n",
    "CUDA_VISIBLE_DEVICES=0 python openai_api_server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd764293-30e5-4860-a92f-ff31b45f3697",
   "metadata": {},
   "source": [
    "- 正常启动日志"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c16437-2796-4950-8f12-801d35f8d6c3",
   "metadata": {},
   "source": [
    "```\n",
    "# CUDA_VISIBLE_DEVICES=0 python openai_api_server.py\n",
    "INFO 08-03 13:26:47 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/root/autodl-tmp/glm-4-9b-chat', speculative_config=None, tokenizer='/root/autodl-tmp/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/root/autodl-tmp/glm-4-9b-chat, use_v2_block_manager=False, enable_prefix_caching=False)\n",
    "WARNING 08-03 13:26:47 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
    "INFO 08-03 13:26:48 model_runner.py:680] Starting to load model /root/autodl-tmp/glm-4-9b-chat...\n",
    "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n",
    "Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:00<00:02,  3.07it/s]\n",
    "Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:00<00:02,  2.73it/s]\n",
    "Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:01<00:02,  2.60it/s]\n",
    "Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:01<00:02,  2.55it/s]\n",
    "Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:01<00:01,  2.51it/s]\n",
    "Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:02<00:01,  2.50it/s]\n",
    "Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:02<00:01,  2.60it/s]\n",
    "Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:03<00:00,  2.60it/s]\n",
    "Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:03<00:00,  2.55it/s]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:03<00:00,  2.51it/s]\n",
    "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:03<00:00,  2.56it/s]\n",
    "\n",
    "INFO 08-03 13:26:52 model_runner.py:692] Loading model weights took 17.5635 GB\n",
    "INFO 08-03 13:26:53 gpu_executor.py:102] # GPU blocks: 2962, # CPU blocks: 6553\n",
    "INFO:     Started server process [2335]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b6529-f202-4dc5-a0d0-cfbac11b33ad",
   "metadata": {},
   "source": [
    "### 显存使用情况\n",
    "- 使用20G显存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd309c7-435a-40f4-91a7-4a83cde175a8",
   "metadata": {},
   "source": [
    "```\n",
    "# nvidia-smi\n",
    "Sat Aug  3 13:50:02 2024\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:B1:00.0 Off |                  Off |\n",
    "| 31%   27C    P8             30W /  450W |   20405MiB /  24564MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa59b4-dc1b-446f-9578-834ee7ecebcb",
   "metadata": {},
   "source": [
    "### 请求访问"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7cf84-944d-4171-8213-e625283a5c66",
   "metadata": {},
   "source": [
    "```\n",
    "# python openai_api_request.py\n",
    "ChatCompletion(id='chatcmpl-yGBEKQAw9qWAwd1R3BPhymc6njmxH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='喵喵喵，我是人工智能助手，很高兴为您服务！有什么可以帮助您的吗？', role='assistant', function_call=None, tool_calls=None))], created=1722664484, model='glm-4', object='chat.completion', service_tier=None, system_fingerprint='fp_j98jzQ2E4', usage=CompletionUsage(completion_tokens=19, prompt_tokens=47, total_tokens=66))\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a363d3-ac29-47fd-abdc-b575d047022d",
   "metadata": {},
   "source": [
    "### 请求API解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a2bf9-ad58-4353-aca1-46d4b390099a",
   "metadata": {},
   "source": [
    "- 依赖安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7ebf7-c132-4ad5-97fc-5188733c5117",
   "metadata": {},
   "source": [
    "```\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd0a0d-29fa-46b7-9c9e-38143bea25d1",
   "metadata": {},
   "source": [
    "- 对话代码示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0082af-ddc9-4e99-82bc-325d0251453e",
   "metadata": {},
   "source": [
    "```\n",
    "from openai import OpenAI\n",
    "\n",
    "base_url = \"http://127.0.0.1:8000/v1/\"\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068ab26-0aec-4ddd-ad4f-764a83e51b7c",
   "metadata": {},
   "source": [
    "```\n",
    "def simple_chat(use_stream=False):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"请在你输出的时候都带上“喵喵喵”三个字，放在开头。\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你是谁\"\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",\n",
    "        messages=messages,\n",
    "        stream=use_stream,\n",
    "        max_tokens=256,\n",
    "        temperature=0.4,\n",
    "        presence_penalty=1.2,\n",
    "        top_p=0.8,\n",
    "    )\n",
    "    if response:\n",
    "        if use_stream:\n",
    "            for chunk in response:\n",
    "                print(chunk)\n",
    "        else:\n",
    "            print(response)\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_chat(use_stream=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c53e5-a54a-4033-82c3-d501c1411e22",
   "metadata": {},
   "source": [
    "### 服务启动代码关键配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73f94b-bd33-4e75-8b61-13e7b0cc982d",
   "metadata": {},
   "source": [
    "- 模型配置路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbfd06-8d17-4955-b7f7-6bc2a7d7e67b",
   "metadata": {},
   "source": [
    "```\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/glm-4-9b-chat')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864d525-94c9-4682-a3d9-6a6c89992937",
   "metadata": {},
   "source": [
    "- vllm部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3defa3c-e37a-4b64-b282-3e33c3e7bcc9",
   "metadata": {},
   "source": [
    "```\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    engine_args = AsyncEngineArgs(\n",
    "        model=MODEL_PATH,\n",
    "        tokenizer=MODEL_PATH,\n",
    "        # 如果你有多张显卡，可以在这里设置成你的显卡数量\n",
    "        tensor_parallel_size=1,\n",
    "        dtype=\"bfloat16\",\n",
    "        trust_remote_code=True,\n",
    "        # 占用显存的比例，请根据你的显卡显存大小设置合适的值，例如，如果你的显卡有80G，您只想使用24G，请按照24/80=0.3设置\n",
    "        gpu_memory_utilization=0.9,\n",
    "        enforce_eager=True,\n",
    "        worker_use_ray=False,\n",
    "        engine_use_ray=False,\n",
    "        disable_log_requests=True,\n",
    "        max_model_len=MAX_MODEL_LENGTH,\n",
    "    )\n",
    "    engine = AsyncLLMEngine.from_engine_args(engine_args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fcfe2-c334-4bb9-a552-e4bc6f4ea859",
   "metadata": {},
   "source": [
    "- FastAPI提供接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca9a40-8311-4953-953a-448ee7622649",
   "metadata": {},
   "source": [
    "```\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c6a0f-f8a7-4f23-8015-4dc191746216",
   "metadata": {},
   "source": [
    "```\n",
    "uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee79ed",
   "metadata": {},
   "source": [
    "GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本。 在语义、数学、推理、代码和知识等多方面的数据集测评中， GLM-4-9B 及其人类偏好对齐的版本 GLM-4-9B-Chat 均表现出超越 Llama-3-8B 的卓越性能。除了能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。本代模型增加了多语言支持，支持包括日语，韩语，德语在内的 26 种语言。我们还推出了支持 1M 上下文长度（约 200 万中文字符）的 GLM-4-9B-Chat-1M 模型和基于 GLM-4-9B 的多模态模型 GLM-4V-9B。GLM-4V-9B 具备 1120 * 1120 高分辨率下的中英双语多轮对话能力，在中英文综合能力、感知推理、文字识别、图表理解等多方面多模态评测中，GLM-4V-9B 表现出超越 GPT-4-turbo-2024-04-09、Gemini 1.0 Pro、Qwen-VL-Max 和 Claude 3 Opus 的卓越性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
