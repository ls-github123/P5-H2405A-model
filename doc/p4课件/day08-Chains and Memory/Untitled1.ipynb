{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2ff30d-c296-4d1e-86a5-b2aff8f911c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m在2024年NBA总决赛的最后一场比赛中，丹佛掘金在主场以94-89击败热火，以总比分4-1成功夺得NBA总冠军。这是球队历史上第一次获得总冠军，也是球队自1976年进入NBA以来的最佳成绩。丹佛掘金队的成功主要归功于他们的领袖球员约基奇的出色表现。\n",
      "在总决赛的五场比赛中，约基奇展现出了惊人的统治力。他场均贡献30.2分、14个篮板和7.2次助攻，成为球队得分、篮板和助攻的核心。约基奇在进攻端展现出了全面的技术，他的得分能力和篮板能力让热火队无可奈何。同时，他还展现出了出色的组织能力，为球队创造了很多得分机会。\n",
      "在总决赛的最后一场比赛中，约基奇更是发挥出色。他在关键时刻承担责任，不仅在进攻端贡献了关键得分，还在防守端起到了重要作用。他的领导能力和稳定性为球队赢得了决胜的胜利。\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m在2024年NBA总决赛中，丹佛掘金以4-1的总比分击败热火，首次夺得NBA总冠军。球队的成功主要得益于领袖球员约基奇在整个系列赛中的出色表现。约基奇在五场比赛中场均得到30.2分、14个篮板和7.2次助攻，成为球队的得分、篮板和助攻核心。他在最后一场比赛中的关键发挥尤其突出，展现了卓越的领导力和稳定性，为球队赢得冠军立下了汗马功劳。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '在2024年NBA总决赛的最后一场比赛中，丹佛掘金在主场以94-89击败热火，以总比分4-1成功夺得NBA总冠军。这是球队历史上第一次获得总冠军，也是球队自1976年进入NBA以来的最佳成绩。丹佛掘金队的成功主要归功于他们的领袖球员约基奇的出色表现。\\n在总决赛的五场比赛中，约基奇展现出了惊人的统治力。他场均贡献30.2分、14个篮板和7.2次助攻，成为球队得分、篮板和助攻的核心。约基奇在进攻端展现出了全面的技术，他的得分能力和篮板能力让热火队无可奈何。同时，他还展现出了出色的组织能力，为球队创造了很多得分机会。\\n在总决赛的最后一场比赛中，约基奇更是发挥出色。他在关键时刻承担责任，不仅在进攻端贡献了关键得分，还在防守端起到了重要作用。他的领导能力和稳定性为球队赢得了决胜的胜利。\\n约基奇荣获总决赛最有价值球员（MVP）毫无悬念。他在总决赛中的出色表现让他成为了不可或缺的球队核心，也让他获得了职业生涯中的首个总冠军。这一荣誉不仅是对他个人努力的认可，也是对他带领球队取得成功的肯定。\\n随着约基奇的崛起，丹佛掘金队在过去几个赛季中逐渐崭露头角。他的全面发展和领导能力使他成为了球队的核心和灵魂人物。通过这次总决赛的胜利，约基奇不仅实现了自己的篮球梦想，也为球队带来了无比的荣耀。\\n约基奇带领丹佛掘金赢得2024年NBA总冠军，并凭借出色的表现获得总决赛最有价值球员（MVP）的荣誉。他在总决赛期间的统治力和全面能力使他成为球队的核心，同时也展现了他的领导才能。这次胜利不仅是约基奇个人职业生涯的里程碑，也是丹佛掘金队迈向更高荣耀的关键一步。随着约基奇的领导，丹佛掘金队有望在未来继续取得更多的成功。',\n",
       " 'output': '在2024年NBA总决赛中，丹佛掘金以4-1的总比分击败热火，首次夺得NBA总冠军。球队的成功主要得益于领袖球员约基奇在整个系列赛中的出色表现。约基奇在五场比赛中场均得到30.2分、14个篮板和7.2次助攻，成为球队的得分、篮板和助攻核心。他在最后一场比赛中的关键发挥尤其突出，展现了卓越的领导力和稳定性，为球队赢得冠军立下了汗马功劳。'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "llm = Tongyi()\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "\n",
    "\n",
    "\n",
    "# 第一个任务\n",
    "def transform_func(inputs:dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    shortened_text = \"\\n\".join(text.split(\"\\n\")[:3])\n",
    "    return {\"output_text\":shortened_text}\n",
    "\n",
    "#文档转换链\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables=[\"output_text\"],\n",
    "    transform=transform_func\n",
    ")\n",
    "\n",
    "# 第二个任务\n",
    "template = \"\"\"对下面的文字进行总结:\n",
    "{output_text}\n",
    "\n",
    "总结:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"output_text\"],\n",
    "    template=template\n",
    ")\n",
    "llm_chain = LLMChain(\n",
    "    llm = Tongyi(),\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "#使用顺序链连接起来\n",
    "squential_chain = SimpleSequentialChain(\n",
    "    chains=[transform_chain,llm_chain],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "with open(\"doc/NBA新闻.txt\",encoding='utf-8') as f:\n",
    "    letters = f.read()\n",
    "squential_chain.invoke(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62610c9f-fc7c-4618-9895-4ca34cfc562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在2024年NBA总决赛中，丹佛掘金以4-1的总比分击败热火，首次夺得总冠军。球队的成功主要得益于领袖球员约基奇在总决赛中的出色表现，他场均贡献30.2分、14个篮板和7.2次助攻，并在最后一场比赛的关键时刻发挥了决定性作用。\n",
      "\n",
      "在2024年NBA总决赛中，丹佛掘金以4-1的总比分击败了热火，首次夺得了总冠军。这一胜利主要归功于球队领袖尼古拉·约基奇在总决赛中的卓越表现。他场均贡献了30.2分、14个篮板和7.2次助攻，并在关键的第五场比赛中发挥了决定性的作用。约基奇毫无悬念地荣获了总决赛最有价值球员（MVP）的称号。他的全面发展与领导能力不仅巩固了他在球队中的核心地位，也为他赢得了职业生涯的第一个总冠军。这次胜利不仅是对约基奇个人努力的认可，也是对他带领球队取得成功的肯定。随着约基奇的崛起，丹佛掘金在过去几个赛季中逐渐崭露头角，这次的总冠军不仅是约基奇个人职业生涯的重要里程碑，也是丹佛掘金队迈向更高荣耀的关键一步。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载问答\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=400, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "prompt_template = \"\"\"对以下文字做简洁的总结:\n",
    "{text}\n",
    "简洁的总结:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "refine_template = (\n",
    "    \"你的任务是产生最终摘要\\n\"\n",
    "    \"我们已经提供了一个到某个特定点的现有回答:{existing_answer}\\n\"\n",
    "    \"我们有机会通过下面的一些更多上下文来完善现有的回答(仅在需要时使用).\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"根据新的上下文，用中文完善原始回答.\\n\"\n",
    "    \"如果上下文没有用处，返回原始回答。\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt = refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key = \"documents\",\n",
    "    output_key = \"output_text\",\n",
    ")\n",
    "result = chain.invoke({\"documents\":split_docs})\n",
    "# print(result)\n",
    "# print(result[\"output_text\"])\n",
    "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a518e7-a9f9-4017-b281-751251975016",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 66\u001b[0m\n\u001b[1;32m     58\u001b[0m map_reduce_chain \u001b[38;5;241m=\u001b[39m MapReduceDocumentsChain(\n\u001b[1;32m     59\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mmap_chain,\n\u001b[1;32m     60\u001b[0m     document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     61\u001b[0m     reduce_documents_chain\u001b[38;5;241m=\u001b[39mreduce_final_chain,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# 默认使用了hugging face上的 GPT-2 模型，下载有问题会报错\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mmap_reduce_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    151\u001b[0m     inputs,\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py:236\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    231\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    232\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    235\u001b[0m ]\n\u001b[0;32m--> 236\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n\u001b[1;32m    240\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py:239\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    219\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    223\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collapse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[1;32m    243\u001b[0m         docs\u001b[38;5;241m=\u001b[39mresult_docs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    244\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py:284\u001b[0m, in \u001b[0;36mReduceDocumentsChain._collapse\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m docs\n\u001b[1;32m    283\u001b[0m length_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mprompt_length\n\u001b[0;32m--> 284\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mlength_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collapse_docs_func\u001b[39m(docs: List[Document], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    288\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    289\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:226\u001b[0m, in \u001b[0;36mStuffDocumentsChain.prompt_length\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    225\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/llm.py:384\u001b[0m, in \u001b[0;36mLLMChain._get_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_language_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/base.py:273\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input will fit in a model's context window.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/base.py:260\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ordered ids of the tokens in a text.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m            in the text.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/base.py:51\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/base.py:45\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPT2TokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2094\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2099\u001b[0m     )\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.reduce import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#load pdf\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=400, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "#map chain\n",
    "map_template = \"\"\"对以下文字做简洁的总结:\n",
    "\"{content}\"\n",
    "简洁的总结:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# 定义任务\n",
    "map_chain = LLMChain(\n",
    "    llm=llm, #llm\n",
    "    prompt=map_prompt,\n",
    ")\n",
    "\n",
    "#reduce chain\n",
    "reduce_template = \"\"\"以下是一个摘要集合:\n",
    "{doc_summaries}\n",
    "将上述摘要与所有关键细节进行总结.\n",
    "总结:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# 定义reduce任务\n",
    "reduce_chain = LLMChain(\n",
    "    prompt=reduce_prompt,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# stuff chain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"doc_summaries\",\n",
    ")\n",
    "\n",
    "# reduce chain\n",
    "reduce_final_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=stuff_chain,\n",
    "    #超过4000个token就会切入到下一个stuff_chain\n",
    "    collapse_documents_chain=stuff_chain,\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# map reduce chain\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    document_variable_name=\"content\",\n",
    "    reduce_documents_chain=reduce_final_chain,\n",
    ")\n",
    "\n",
    "# 默认使用了hugging face上的 GPT-2 模型，下载有问题会报错\n",
    "\n",
    "summary = map_reduce_chain.invoke(split_docs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89c1423d-f4fa-4227-a7f9-d14bc4ba3bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=RegexParser(regex='(.*?)\\\\nScore: (\\\\d*)', output_keys=['answer', 'score']), template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\"), llm=Tongyi(client=<class 'dashscope.aigc.generation.Generation'>, dashscope_api_key='sk-e7646bb1ca104d1dbb0f1560d848e75e')) document_variable_name='context' rank_key='score' answer_key='answer' metadata_keys=['source'] return_intermediate_steps=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='在2024年NBA总决赛的最后一场比赛中，丹佛掘金在主场以94-89击败热火，以总比分4-1成功夺得NBA总冠军。这是球队历史上第一次获得总冠军，也是球队自1976年进入NBA以来的最佳成绩。丹佛掘金队的成功主要归功于他们的领袖球员约基奇的出色表现。', metadata={'source': 'doc/NBA新闻.txt'}),\n",
       "  Document(page_content='在总决赛的五场比赛中，约基奇展现出了惊人的统治力。他场均贡献30.2分、14个篮板和7.2次助攻，成为球队得分、篮板和助攻的核心。约基奇在进攻端展现出了全面的技术，他的得分能力和篮板能力让热火队无可奈何。同时，他还展现出了出色的组织能力，为球队创造了很多得分机会。', metadata={'source': 'doc/NBA新闻.txt'}),\n",
       "  Document(page_content='在总决赛的最后一场比赛中，约基奇更是发挥出色。他在关键时刻承担责任，不仅在进攻端贡献了关键得分，还在防守端起到了重要作用。他的领导能力和稳定性为球队赢得了决胜的胜利。\\n约基奇荣获总决赛最有价值球员（MVP）毫无悬念。他在总决赛中的出色表现让他成为了不可或缺的球队核心，也让他获得了职业生涯中的首个总冠军。这一荣誉不仅是对他个人努力的认可，也是对他带领球队取得成功的肯定。', metadata={'source': 'doc/NBA新闻.txt'}),\n",
       "  Document(page_content='随着约基奇的崛起，丹佛掘金队在过去几个赛季中逐渐崭露头角。他的全面发展和领导能力使他成为了球队的核心和灵魂人物。通过这次总决赛的胜利，约基奇不仅实现了自己的篮球梦想，也为球队带来了无比的荣耀。', metadata={'source': 'doc/NBA新闻.txt'}),\n",
       "  Document(page_content='约基奇带领丹佛掘金赢得2024年NBA总冠军，并凭借出色的表现获得总决赛最有价值球员（MVP）的荣誉。他在总决赛期间的统治力和全面能力使他成为球队的核心，同时也展现了他的领导才能。这次胜利不仅是约基奇个人职业生涯的里程碑，也是丹佛掘金队迈向更高荣耀的关键一步。随着约基奇的领导，丹佛掘金队有望在未来继续取得更多的成功。', metadata={'source': 'doc/NBA新闻.txt'})],\n",
       " 'question': '中文回答这篇文章的主要内容是什么？',\n",
       " 'source': 'doc/NBA新闻.txt',\n",
       " 'intermediate_steps': [{'answer': '这篇文章的主要内容是关于2024年NBA总决赛的最后一场比赛，丹佛掘金在主场以94-89击败了热火，以总比分4-1赢得了NBA总冠军。这是丹佛掘金队历史上首次获得总冠军，也是自1976年加入NBA以来的最佳成绩。球队的成功很大程度上归功于领袖球员约基奇的出色表现。',\n",
       "   'score': '100'},\n",
       "  {'answer': '这篇文章的主要内容是描述了约基奇在总决赛五场比赛中的表现。他场均得到30.2分、14个篮板和7.2次助攻，是球队在得分、篮板和助攻方面的核心。约基奇不仅展现了出色的得分和篮板能力，还表现出了优秀的组织能力，为球队创造了多次得分机会。',\n",
       "   'score': '100'},\n",
       "  {'answer': '这篇文章的主要内容是约基奇在总决赛最后一场比赛中的出色表现，他不仅在进攻端有关键得分，在防守端也起到了重要作用。由于他的领导能力和稳定性，球队赢得了最终的胜利。因此，约基奇被评为总决赛最有价值球员（MVP），并帮助球队赢得了首个总冠军。',\n",
       "   'score': '90'},\n",
       "  {'answer': '这篇文章的主要内容是关于约基奇的崛起以及他对丹佛掘金队的影响。他的全面技能和领导力让他成为球队的关键人物，并且通过赢得总决赛，约基奇实现了个人梦想并为球队赢得了荣誉。',\n",
       "   'score': '90'},\n",
       "  {'answer': '这篇文章的主要内容是约基奇带领丹佛掘金赢得了2024年NBA总冠军，并且因为他的出色表现获得了总决赛MVP的荣誉。约基奇在总决赛中展现了统治力、全面能力和领导才能，这对他的个人职业生涯和丹佛掘金队都是一个重要的里程碑。约基奇的领导预示着球队未来可能会有更多的成功。',\n",
       "   'score': '100'}],\n",
       " 'output_text': '这篇文章的主要内容是关于2024年NBA总决赛的最后一场比赛，丹佛掘金在主场以94-89击败了热火，以总比分4-1赢得了NBA总冠军。这是丹佛掘金队历史上首次获得总冠军，也是自1976年加入NBA以来的最佳成绩。球队的成功很大程度上归功于领袖球员约基奇的出色表现。'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.reduce import ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "#load\n",
    "# loader = PyPDFLoader(\"doc/demo.pdf\")\n",
    "\n",
    "loader = TextLoader(\"doc/NBA新闻.txt\",encoding='utf-8')\n",
    "docs = loader.load()\n",
    "#split\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",chunk_size=200, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    llm, \n",
    "    chain_type=\"map_rerank\", \n",
    "    metadata_keys=['source'], \n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "print(chain)\n",
    "query = \"中文回答这篇文章的主要内容是什么？\"\n",
    "result = chain.invoke({\"input_documents\":split_docs,\"question\":query})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f275a-b82a-46f9-8a39-a2b53de7e392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
